{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38940677-f41f-4b93-8b77-64dd34922131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 1:\n",
    "# The decision tree classifier is a machine learning algorithm used for both classification and regression tasks.\n",
    "# It creates a tree-like model of decisions and their possible consequences.\n",
    "# The algorithm learns from labeled training data and builds a tree structure where each internal node represents a feature or attribute\n",
    "\n",
    "# working\n",
    "# 1.Data preparation:The algorithm requires a labeled dataset where each instance has a set of features and a corresponding class label.\n",
    "# 2.Building the Tree: The algorithm starts by selecting the best feature to split the dataset based on certain criteria, \n",
    "#   such as information gain or Gini impurity.\n",
    "# 3.Splitting Nodes: The selected feature is used to split the dataset into subsets based on its possible attribute values. \n",
    "# 4.Recursive Splitting: The splitting process is recursively applied to each subset, \n",
    "#   creating more branches and nodes in the tree until a stopping criterion is met.\n",
    "# 5.Making Predictions: To make predictions on new, unseen instances, \n",
    "#   the algorithm starts at the root node and follows the decision path based on the feature values of the instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d49b6516-59c7-4171-9e8c-66fff833e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 2:\n",
    "# Entropy and Information Gain: The decision tree algorithm uses the concepts of entropy and information gain \n",
    "#    to measure the impurity or disorder in a set of instances.\n",
    "# 1.Entropy is a measure of uncertainty or randomness in a set\n",
    "# 2.information gain quantifies the reduction in entropy achieved by splitting the dataset based on a particular feature.\n",
    "\n",
    "# Mathematically, entropy is defined as:\n",
    "#  H(S) = - Σ (p(i) * log₂(p(i)))\n",
    "# Entropy represents the average amount of information required to determine the class label of an instance in the set. \n",
    "# If all instances in the set belong to the same class, entropy is 0 (perfectly pure set). \n",
    "# If the instances are evenly distributed across different classes, entropy is at its maximum (impure set).\n",
    "\n",
    "# Mathematically, information gain is given by:\n",
    "#  Gain(S, F) = H(S) - Σ ((|S_v| / |S|) * H(S_v))\n",
    "#  Information gain is defined as the difference between the entropy of the parent set (before splitting) \n",
    "#  and the weighted average of the entropies of the child subsets (after splitting), based on a specific feature.\n",
    "\n",
    "#  Information gain is defined as the difference between the entropy of the parent set (before splitting) \n",
    "# and the weighted average of the entropies of the child subsets (after splitting), based on a specific feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce83c09-95cb-436f-91d4-2409e4dd4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 3 :\n",
    "# It deals classification with problem with same steps\n",
    "# # working\n",
    "# 1.Data preparation:The algorithm requires a labeled dataset where each instance has a set of features and a corresponding class label.\n",
    "# 2.Building the Tree: The algorithm starts by selecting the best feature to split the dataset based on certain criteria, \n",
    "#   such as information gain or Gini impurity.\n",
    "# 3.Splitting Nodes: The selected feature is used to split the dataset into subsets based on its possible attribute values. \n",
    "# 4.Recursive Splitting: The splitting process is recursively applied to each subset, \n",
    "#   creating more branches and nodes in the tree until a stopping criterion is met.\n",
    "# 5.Making Predictions: To make predictions on new, unseen instances, \n",
    "#   the algorithm starts at the root node and follows the decision path based on the feature values of the instance. \n",
    "\n",
    "# The decision tree classifier is capable of capturing complex decision boundaries and can handle both numerical and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eaeee08-08af-40c8-9026-4525e1591e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 4 :\n",
    "# Entropy and Information Gain: The decision tree algorithm uses the concepts of entropy and information gain \n",
    "#    to measure the impurity or disorder in a set of instances.\n",
    "# 1.Entropy is a measure of uncertainty or randomness in a set\n",
    "# 2.information gain quantifies the reduction in entropy achieved by splitting the dataset based on a particular feature.\n",
    "\n",
    "# Mathematically, entropy is defined as:\n",
    "#  H(S) = - Σ (p(i) * log₂(p(i)))\n",
    "# Entropy represents the average amount of information required to determine the class label of an instance in the set. \n",
    "# If all instances in the set belong to the same class, entropy is 0 (perfectly pure set). \n",
    "# If the instances are evenly distributed across different classes, entropy is at its maximum (impure set).\n",
    "\n",
    "# Mathematically, information gain is given by:\n",
    "#  Gain(S, F) = H(S) - Σ ((|S_v| / |S|) * H(S_v))\n",
    "#  Information gain is defined as the difference between the entropy of the parent set (before splitting) \n",
    "#  and the weighted average of the entropies of the child subsets (after splitting), based on a specific feature.\n",
    "\n",
    "#  Information gain is defined as the difference between the entropy of the parent set (before splitting) \n",
    "# and the weighted average of the entropies of the child subsets (after splitting), based on a specific feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838d349f-43dc-422d-a27b-820b5a1f3bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 5 :\n",
    "# Confusion Matrix\n",
    "# It is a 2x2 Matrix with actual values and predicted values\n",
    "# A confusion matrix is a table that summarizes the performance of a classification model by \n",
    "# displaying the counts of true positive, true negative, false positive, and false negative predictions. \n",
    "# It is a useful tool for evaluating the performance of a classification model \n",
    "#                Predicted\n",
    "#                     Positive   Negative\n",
    "#   Actual   Positive    TP         FN\n",
    "#            Negative    FP         TN\n",
    "\n",
    "# where\n",
    "# TP (True Positive) represents the number of instances correctly predicted as positive by the model.\n",
    "# TN (True Negative) represents the number of instances correctly predicted as negative by the model.\n",
    "# FP (False Positive) represents the number of instances incorrectly predicted as positive when they are actually negative. \n",
    "# FN (False Negative) represents the number of instances incorrectly predicted as negative when they are actually positive.\n",
    "\n",
    "# It tells about the \n",
    "# 1.Accuracy: The overall accuracy of the model can be calculated as (TP + TN) / (TP + TN + FP + FN). \n",
    "#   It measures the proportion of correctly classified instances out of the total.\n",
    "# 2.Precision: Precision measures the proportion of correctly predicted positive instances out of all instances predicted as positive.\n",
    "#   It is calculated as TP / (TP + FP)\n",
    "# 3.Recall: Recall measures the proportion of correctly predicted positive instances out of all actual positive instances. \n",
    "#          It is calculated as TP / (TP + FN). \n",
    "# 4.F1 Score: The F1 score is the harmonic mean of precision and recall.\n",
    "#   It provides a single metric that balances both precision and recall. \n",
    "#   It is calculated as 2 * (Precision * Recall) / (Precision + Recall).\n",
    "\n",
    "# The confusion matrix allows you to assess the performance of a classification model \n",
    "# in terms of its ability to correctly classify instances into different classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0138afab-d9ff-4342-a3fc-f32d36fd72f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 6 :\n",
    "# For example \n",
    "#                  Predicted\n",
    "#                  Positive   Negative\n",
    "# Actual   Positive    TP-3       FP-2\n",
    "#          Negative    FN-1       TN-1\n",
    "\n",
    "# These values are taken from dataset where we have actual y and predicted y\n",
    "\n",
    "# where\n",
    "# TP (True Positive) represents the number of instances correctly predicted as positive by the model.\n",
    "# TN (True Negative) represents the number of instances correctly predicted as negative by the model.\n",
    "# FP (False Positive) represents the number of instances incorrectly predicted as positive when they are actually negative. \n",
    "# FN (False Negative) represents the number of instances incorrectly predicted as negative when they are actually positive.\n",
    "\n",
    "# 1.Accuracy:(TP + TN) / (TP + TN + FP + FN)\n",
    "# ans = 3+1 / 3+1+2+1 = 0.57\n",
    "# 2.Precision: TP / (TP + FP)\n",
    "# ans = 3/5 = 0.6\n",
    "# 3.Recall :TP / (TP + FN). \n",
    "# ans = 3/4 = 0.75\n",
    "# 4.F1 Score : 2 * (Precision * Recall) / (Precision + Recall).\n",
    "# ans = 2 *  (0.6 X 0.75)/(0.6 + 0.75) = 0.66 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "204465ed-17a0-4af5-b1c7-a4f5524a7278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 7 :\n",
    "# why selecting the right evaluation metric is important and how it can be done:\n",
    "# 1.Reflecting the Problem's Context: The choice of evaluation metric should align with the problem's context and objectives. \n",
    "# 2.Handling Class Imbalance: Class imbalance occurs when one class has significantly more instances than the other\n",
    "\n",
    "# It can be done by \n",
    "# 1.Understand the Problem: Gain a clear understanding of the problem's context, objectives, and the specific requirements\n",
    "# 2.Analyze the Data: Analyze the characteristics of the dataset, including class distribution and class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4b53b9d-4a77-45bc-b55f-3595f6771e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 8 :\n",
    "# for example we have check the  mail is spam or not\n",
    "# true positive case :\n",
    "#  Mail --- spam\n",
    "#  Model --- spam \n",
    "\n",
    "# False positve case:\n",
    "# Mail -- not a spam\n",
    "# Model -- spam \n",
    "# This will be a blunder\n",
    "\n",
    "# False Negative\n",
    "# Mail -- spam\n",
    "# Model --- not a spam\n",
    "# this is ok \n",
    "\n",
    "# In this we have to reduce the False Positive case \n",
    "# hence precesion works well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28139389-0b79-4222-8029-e5f185de119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 9:\n",
    "# for example we have check the person is sufferring from diabetes or not\n",
    "# true positive case :\n",
    "#  Actual --- Diabetes\n",
    "#  Model --- Diabetes \n",
    "\n",
    "# False Negative case:\n",
    "# Actual -- Diabetes \n",
    "# Model -- not Diabetic\n",
    "# This will be a blunder\n",
    "\n",
    "# False Positive case\n",
    "# Actual -- Not Diabetic\n",
    "# Model --- Diabetic\n",
    "# this is ok \n",
    "\n",
    "# In this we have to reduce the False Po case \n",
    "# hence precesion works well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
